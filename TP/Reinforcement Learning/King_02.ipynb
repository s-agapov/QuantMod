{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import talib\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "from data_provider import read_data\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human', 'file', 'none']}\n",
    "\n",
    "    def __init__(self, data, lead_window, lag_window):\n",
    "        super(TradingEnv, self).__init__()\n",
    "\n",
    "        self.prices = data['C'].values\n",
    "        self.initial_balance = 10000\n",
    "        self.commission = 0.00075\n",
    "        self.contract = 10\n",
    "        self.max_contracts = 2\n",
    "        self.open_position = 0\n",
    "        self.profit = 0\n",
    "    \n",
    "        self.offset = lag_window\n",
    "        self.fast = talib.MA(self.prices, lead_window)\n",
    "        self.slow = talib.MA(self.prices, lag_window)\n",
    "        ma_diff =  self.fast - self.slow\n",
    "        ma_diff[:self.offset] = 0\n",
    "        bins = np.quantile(ma_diff[self.offset:], np.arange(50)/100)\n",
    "        self.ma_states = np.digitize(ma_diff, bins)\n",
    "        self.bins = bins\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.max_contracts * 2 + 1)\n",
    " #       self.observation_space = spaces.Box(low=0, high=1, shape=(2,), dtype=np.float16)   \n",
    "        n_states = (len(bins) + 1) * (self.max_contracts + 1) + 1\n",
    "        self.observation_space = spaces.Discrete(n_states)      \n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.ma_states[self.current_step] * (self.max_contracts + 1) +  int(self.shares_held/self.contract)\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.offset\n",
    "        self.balance = self.initial_balance\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.shares_held = 0.0\n",
    "        self.trades = 0\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        \n",
    "        amount = (action + 1) // 2\n",
    "        action_type = (action + 1) % 2 + 1 \n",
    "    \n",
    "        current_price = self.prices[self.current_step]        \n",
    "        if amount > 0:\n",
    "            if action_type == 1 and self.shares_held/self.contract < self.max_contracts:\n",
    "                price = current_price * buy_correction\n",
    "                shares_boughten = self.contract * amount\n",
    "                self.shares_held += shares_boughten\n",
    "                cost =  shares_boughten * price * (1 + self.commission)\n",
    "                self.balance = self.balance - cost\n",
    "                self.open_position = self.open_position + cost \n",
    "\n",
    "            elif action_type == 2 and self.shares_held > 0:\n",
    "                price = current_price * sell_correction\n",
    "                shares_sold = min(self.contract * amount, self.shares_held)\n",
    "                cash = shares_sold * price *(1 - self.commission) \n",
    "                self.balance = self.balance + cash\n",
    "                cost = self.open_position/self.shares_held * shares_sold\n",
    "                self.profit = cash - cost\n",
    "                self.open_position = max(self.open_position - cost, 0)\n",
    "                self.shares_held = self.shares_held - shares_sold\n",
    "                self.trades = self.trades + 1\n",
    "            \n",
    "        self.net_worth = self.balance + self.shares_held * current_price * (1 - self.commission) * sell_correction\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "\n",
    "        #delay_modifier = (self.current_step / MAX_STEPS)\n",
    "        #reward = self.balance * delay_modifier + self.current_step\n",
    "        \n",
    "        reward = self.profit\n",
    "        self.profit = 0\n",
    "        done = self.net_worth <= 0 or self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}     \n",
    "    \n",
    "data = read_data('LINK-USDT', '1h', '2018-02-06')\n",
    "MAX_STEPS = data.shape[0]\n",
    "train = round(MAX_STEPS * 0.7)\n",
    "env = TradingEnv(data.iloc[:train], 78, 105)\n",
    "\n",
    "action_space = np.stack([np.arange(6) // 2, np.arange(6) % 2 ], axis = 1)\n",
    "buy_correction =  1.0001\n",
    "sell_correction   = 0.9999\n",
    "\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 Total Reward: -96.86617118280124, epsilon: 0.7609740241938675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 Total Reward: 2.5801791646486496, epsilon: 0.7238518318722612\n",
      "Episode 300 Total Reward: -38.62002783037633, epsilon: 0.6885405517749218\n",
      "Episode 400 Total Reward: -52.263130481284286, epsilon: 0.6549518431310352\n",
      "Episode 500 Total Reward: -49.818102112050894, epsilon: 0.623001674650768\n",
      "Episode 600 Total Reward: -68.80678811407597, epsilon: 0.5926101142981421\n",
      "Episode 700 Total Reward: -4.696403157325914, epsilon: 0.5637011293193063\n",
      "Episode 800 Total Reward: -8.755167277650681, epsilon: 0.5362023960259251\n",
      "Episode 900 Total Reward: -40.26213343914684, epsilon: 0.5100451188578026\n",
      "Episode 1000 Total Reward: -19.690610681450735, epsilon: 0.4851638582720763\n",
      "Episode 1100 Total Reward: -31.909223679025608, epsilon: 0.4614963670284061\n",
      "Episode 1200 Total Reward: -23.056914088125712, epsilon: 0.4389834344605705\n",
      "Episode 1300 Total Reward: -27.063082514300607, epsilon: 0.4175687383448818\n",
      "Episode 1400 Total Reward: -57.01163034117995, epsilon: 0.397198703994826\n",
      "Episode 1500 Total Reward: -65.66238401410062, epsilon: 0.37782237022941423\n",
      "Episode 1600 Total Reward: -39.39818073595046, epsilon: 0.35939126187992837\n",
      "Episode 1700 Total Reward: -82.02536244440883, epsilon: 0.3418592685161015\n",
      "Episode 1800 Total Reward: -37.09251429695037, epsilon: 0.3251825290883371\n",
      "Episode 1900 Total Reward: -35.61343598195051, epsilon: 0.3093193221973638\n",
      "Episode 2000 Total Reward: -59.945486664533824, epsilon: 0.29422996171680904\n",
      "Episode 2100 Total Reward: -61.83442879525062, epsilon: 0.27987669750755967\n",
      "Episode 2200 Total Reward: -42.393090676875424, epsilon: 0.26622362097552177\n",
      "Episode 2300 Total Reward: -10.315190786875384, epsilon: 0.2532365752365069\n",
      "Episode 2400 Total Reward: 74.94394971439937, epsilon: 0.24088306966349723\n",
      "Episode 2500 Total Reward: 23.393220449602488, epsilon: 0.22913219860250397\n",
      "Episode 2600 Total Reward: 35.49681188749958, epsilon: 0.2179545640536697\n",
      "Episode 2700 Total Reward: -55.37365365682535, epsilon: 0.2073222021241763\n",
      "Episode 2800 Total Reward: 12.056849932749596, epsilon: 0.19720851306896112\n",
      "Episode 2900 Total Reward: 1.2126588846539583, epsilon: 0.18758819474422012\n",
      "Episode 3000 Total Reward: -39.4552362642504, epsilon: 0.17843717930721498\n",
      "Episode 3100 Total Reward: -5.765731517750178, epsilon: 0.1697325730040177\n",
      "Episode 3200 Total Reward: -44.48653358001694, epsilon: 0.16145259889455837\n",
      "Episode 3300 Total Reward: -42.145229926350176, epsilon: 0.15357654237168802\n",
      "Episode 3400 Total Reward: -41.69224489940018, epsilon: 0.14608469933795434\n",
      "Episode 3500 Total Reward: 5.878299751549786, epsilon: 0.1389583269104429\n",
      "Episode 3600 Total Reward: -11.927025755875317, epsilon: 0.13217959653035824\n",
      "Episode 3700 Total Reward: 16.84993024064983, epsilon: 0.12573154936003547\n",
      "Episode 3800 Total Reward: 63.009416897341595, epsilon: 0.11959805385579514\n",
      "Episode 3900 Total Reward: 16.54135050223608, epsilon: 0.1137637654104992\n",
      "Episode 4000 Total Reward: -0.4226292785751813, epsilon: 0.10821408796484333\n",
      "Episode 4100 Total Reward: -46.42364523565027, epsilon: 0.10293513749134495\n",
      "Episode 4200 Total Reward: -64.91847006170019, epsilon: 0.09791370725967229\n",
      "Episode 4300 Total Reward: -40.99132665720016, epsilon: 0.09313723479641645\n",
      "Episode 4400 Total Reward: -4.671829321250147, epsilon: 0.0885937704566476\n",
      "Episode 4500 Total Reward: -88.64926446575011, epsilon: 0.0842719475286286\n",
      "Episode 4600 Total Reward: -50.18493336060013, epsilon: 0.08016095379689364\n",
      "Episode 4700 Total Reward: 0.7866279280998931, epsilon: 0.07625050449255102\n",
      "Episode 4800 Total Reward: -36.45194217398355, epsilon: 0.07253081656313645\n",
      "Episode 4900 Total Reward: -61.07515533837515, epsilon: 0.06899258419764652\n",
      "Episode 5000 Total Reward: -0.7690436845750597, epsilon: 0.06562695554552156\n",
      "Episode 5100 Total Reward: -31.825232816925148, epsilon: 0.06242551057133442\n",
      "Episode 5200 Total Reward: 3.2749230048581843, epsilon: 0.05938023998978146\n",
      "Episode 5300 Total Reward: -30.076528052400025, epsilon: 0.056483525228276994\n",
      "Episode 5400 Total Reward: 56.28003305382493, epsilon: 0.05372811936702219\n",
      "Episode 5500 Total Reward: -74.04146524777511, epsilon: 0.051107129008864174\n",
      "Episode 5600 Total Reward: -38.357406698925004, epsilon: 0.04861399703358814\n",
      "Episode 5700 Total Reward: 12.484017194299842, epsilon: 0.04624248619349786\n",
      "Episode 5800 Total Reward: -43.56042614065002, epsilon: 0.04398666350924428\n",
      "Episode 5900 Total Reward: 17.508715332999884, epsilon: 0.041840885426863955\n",
      "Episode 6000 Total Reward: 9.034648921561068, epsilon: 0.039799783698894045\n",
      "Episode 6100 Total Reward: -17.385646454333372, epsilon: 0.03785825195424109\n",
      "Episode 6200 Total Reward: -15.079939698908433, epsilon: 0.03601143292320525\n",
      "Episode 6300 Total Reward: -40.2153004795806, epsilon: 0.03425470628569882\n",
      "Episode 6400 Total Reward: 11.36302404974165, epsilon: 0.03258367711225903\n",
      "Episode 6500 Total Reward: -6.658396037350078, epsilon: 0.03099416486893669\n",
      "Episode 6600 Total Reward: -61.96934007855003, epsilon: 0.02948219295855367\n",
      "Episode 6700 Total Reward: -43.50579443947507, epsilon: 0.02804397877216337\n",
      "Episode 6800 Total Reward: -24.057866886041733, epsilon: 0.02667592422582571\n",
      "Episode 6900 Total Reward: 9.899974163274916, epsilon: 0.02537460675902159\n",
      "Episode 7000 Total Reward: 4.80652771539998, epsilon: 0.024136770772186966\n",
      "Episode 7100 Total Reward: 7.499075659875022, epsilon: 0.022959319481945042\n",
      "Episode 7200 Total Reward: -36.50298554495004, epsilon: 0.02183930717366047\n",
      "Episode 7300 Total Reward: -19.051427238275032, epsilon: 0.020773931831932987\n",
      "Episode 7400 Total Reward: 2.3481708711999545, epsilon: 0.01976052813059393\n",
      "Episode 7500 Total Reward: 42.16134359517496, epsilon: 0.018796560764667743\n",
      "Episode 7600 Total Reward: -27.006505420000003, epsilon: 0.01787961810761722\n",
      "Episode 7700 Total Reward: -84.07631529051253, epsilon: 0.017007406178003785\n",
      "Episode 7800 Total Reward: 8.638603132749957, epsilon: 0.01617774290046898\n",
      "Episode 7900 Total Reward: 51.25107127992494, epsilon: 0.015388552646679548\n",
      "Episode 8000 Total Reward: 67.06283295247496, epsilon: 0.014637861042578653\n",
      "Episode 8100 Total Reward: 22.16862743114997, epsilon: 0.013923790028952152\n",
      "Episode 8200 Total Reward: -24.515461678550015, epsilon: 0.0132445531629527\n",
      "Episode 8300 Total Reward: -45.54523867684169, epsilon: 0.012598451148827156\n",
      "Episode 8400 Total Reward: 24.47703754135625, epsilon: 0.011983867586666072\n",
      "Episode 8500 Total Reward: 39.37772364452497, epsilon: 0.01139926492853967\n",
      "Episode 8600 Total Reward: -55.621344741787496, epsilon: 0.010843180631903565\n",
      "Episode 8700 Total Reward: -9.22971227782503, epsilon: 0.010314223500650822\n",
      "Episode 8800 Total Reward: -31.772816585875006, epsilon: 0.009999310218331782\n",
      "Episode 8900 Total Reward: 70.89274767734999, epsilon: 0.009999310218331782\n",
      "Episode 9000 Total Reward: 47.281864516775, epsilon: 0.009999310218331782\n",
      "Episode 9100 Total Reward: -33.57417941787498, epsilon: 0.009999310218331782\n",
      "Episode 9200 Total Reward: -26.713494795275007, epsilon: 0.009999310218331782\n",
      "Episode 9300 Total Reward: -50.59728517307503, epsilon: 0.009999310218331782\n",
      "Episode 9400 Total Reward: -7.444352916000021, epsilon: 0.009999310218331782\n",
      "Episode 9500 Total Reward: 23.197832230574978, epsilon: 0.009999310218331782\n",
      "Episode 9600 Total Reward: 12.439996630474973, epsilon: 0.009999310218331782\n",
      "Episode 9700 Total Reward: -32.03966396764173, epsilon: 0.009999310218331782\n",
      "Episode 9800 Total Reward: 16.644804936624993, epsilon: 0.009999310218331782\n",
      "Episode 9900 Total Reward: 32.948106409962506, epsilon: 0.009999310218331782\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "Q = np.zeros([number_of_states, number_of_actions])\n",
    "G = 0\n",
    "alpha = 0.1\n",
    "epsilon = 0.8\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 0.9995\n",
    "gamma = 0.999\n",
    "writer = SummaryWriter(comment=\"-trade_env_ma\")\n",
    "\n",
    "for episode in range(1, num_episodes):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        random_for_egreedy = np.random.rand()\n",
    "        if random_for_egreedy > epsilon:    \n",
    "            action = np.argmax(Q[state]) #1\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action) #2\n",
    "\n",
    "        Q[state,action] += (1 - alpha) *  Q[state,action]  + alpha * (reward + gamma * np.max(Q[new_state])) #3\n",
    "        G += reward\n",
    "        state = new_state   \n",
    "    writer.add_scalar(\"reward\", G, episode) \n",
    "    writer.add_scalar(\"trades\", env.trades, episode)      \n",
    "    writer.add_scalar(\"shares\", env.shares_held, episode)      \n",
    "    if epsilon > epsilon_final:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    if episode % 200 == 0:   \n",
    "        Q = Q - Q.mean(axis = 1, keepdims = 1)\n",
    "        \n",
    "    if episode % 100 == 0:\n",
    "        print('Episode {} Total Reward: {}, epsilon: {}'.format(episode,  G, epsilon))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env_val = TradingEnv(data.iloc[train:], 78, 105)\n",
    "state = env_val.reset()\n",
    "profit = []\n",
    "done = False\n",
    "while done != True:\n",
    "    action = np.argmax(Q[state]) #1\n",
    "    state, reward, done, info = env_val.step(action) #2\n",
    "\n",
    "    profit.append(reward)\n",
    "\n",
    "cum_profit = np.array(profit).cumsum()\n",
    "\n",
    "plt.plot(cum_profit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q - Q.mean(axis = 1)\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "qs = scale(Q - Q.mean(axis = 1, keepdims = 1), axis =1).round(2)\n",
    "\n",
    "df = pd.DataFrame(qs)\n",
    "df.columns= ['hold', 'buy_1', 'buy_2', 'sell_1', 'sell_2']\n",
    "\n",
    "ind = [-np.inf] + np.round(env.bins * 100).astype(int).tolist()\n",
    "df.index = ind\n",
    "df.apply(lambda x: df.columns[np.argmax(x)], axis =1)\n",
    "Q.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "egreedy_total = []\n",
    "# PARAMS \n",
    "\n",
    "# Discount on reward\n",
    "gamma = 0.95\n",
    "\n",
    "# Factor to balance the ratio of action taken based on past experience to current situtation\n",
    "learning_rate = 0.5\n",
    "\n",
    "# exploit vs explore to find action\n",
    "# Start with 70% random actions to explore the environment\n",
    "# And with time, using decay to shift to more optimal actions learned from experience\n",
    "\n",
    "egreedy = 0.7\n",
    "egreedy_final = 0.1\n",
    "egreedy_decay = 0.999\n",
    "Q = torch.zeros([number_of_states, number_of_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(arr):\n",
    "    return torch.max(arr, 0)[1]\n",
    "    \n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # resets the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > egreedy:      \n",
    "            random_values = Q[state]    \n",
    "            action = argmax(random_values) \n",
    "            action = action_space[action.item()]\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        if egreedy > egreedy_final:\n",
    "            egreedy *= egreedy_decay\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Filling the Q Table\n",
    "        Q[state, action] += learning_rate*(reward + gamma * torch.max(Q[new_state]) - Q[state,action])\n",
    "        \n",
    "        # Setting new state for next action\n",
    "        state = new_state\n",
    "        \n",
    "        # env.render()\n",
    "        # time.sleep(0.4)\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            egreedy_total.append(egreedy)\n",
    "            if i_episode % 10 == 0:\n",
    "                print('Episode: {} Reward: {} Steps Taken: {}'.format(i_episode,reward, step))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

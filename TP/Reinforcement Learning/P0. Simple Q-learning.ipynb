{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import enum\n",
    "import sys\n",
    "from pylab import plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import talib\n",
    "\n",
    "sys.path.append(\"..\") \n",
    "from data_provider import  read_prices, read_data\n",
    "from tp_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket = 'BTC-USDT'\n",
    "tf = '1h'\n",
    "#tf = '5m'\n",
    "from_date = '2020-01-01'\n",
    "\n",
    "prices = read_data(ticket, tf, from_date)\n",
    "\n",
    "lead_win =12\n",
    "lag_win  = 26\n",
    "fast_ma = talib.MA(prices[\"C\"].values, lead_win)\n",
    "slow_ma = talib.MA(prices[\"C\"].values, lag_win)\n",
    "volume  = prices[\"V\"].values\n",
    "volume_ma =  talib.MA(volume, lag_win)\n",
    "\n",
    "fast_ma = fast_ma[lag_win:]\n",
    "slow_ma = slow_ma[lag_win:]\n",
    "volume  = volume[lag_win:]\n",
    "volume_ma = volume_ma[lag_win:]\n",
    "c_price = prices[\"C\"].values[lag_win:]\n",
    "times = prices[\"T\"].values[lag_win:]\n",
    "\n",
    "xx = fast_ma / slow_ma - 1\n",
    "quantiles = np.quantile(xx, [0.2, 0.45, 0.55, 0.8])\n",
    "ma_bins = [sum(x > quantiles) - 2 for x in xx]\n",
    "               \n",
    "xx = volume / volume_ma - 1\n",
    "quantiles = np.quantile(xx, [0.2, 0.45, 0.55, 0.8])\n",
    "volume_bins = [sum(x > quantiles) for x in xx]\n",
    "volume_bins\n",
    "\n",
    "df_states = pd.DataFrame()\n",
    "df_states['T']      = times\n",
    "df_states['price']  = c_price\n",
    "df_states['ma']     = ma_bins\n",
    "df_states['volume'] = volume_bins\n",
    "\n",
    "prices = prices.iloc[lag_win:]\n",
    "prices = prices.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(enum.Enum):\n",
    "    Skip = 0\n",
    "    Buy = 1\n",
    "    Close = 2\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, states, bars_count, commission_perc, reward_on_close=True,):\n",
    "        assert isinstance(bars_count, int)\n",
    "        assert bars_count > 0\n",
    "        \n",
    "        self.bars_count = bars_count\n",
    "        self.commission_perc = commission_perc\n",
    "        self.reward_on_close = reward_on_close\n",
    "        self.reset(states, bars_count)\n",
    "        \n",
    "    def reset(self, states, offset):\n",
    "        assert offset >= self.bars_count - 1\n",
    "        self.equity = 0\n",
    "        self.market_position = False\n",
    "        self.open_price = 0.0\n",
    "        self._states = states\n",
    "        self._offset = offset\n",
    "\n",
    "    @property\n",
    "    def space_type(self):\n",
    "        return gym.spaces.Box(low=-2, high=4, shape = [3], dtype = np.int32)\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return 4*4*2\n",
    "    \n",
    "    def encode(self):\n",
    "        res = []\n",
    "        row = self._states.iloc[self._offset]\n",
    "        res.append(row['ma'])\n",
    "        res.append(row['volume'])\n",
    "        res.append(self.market_position)\n",
    "        return ' '.join(map(str, res))\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in our price, adjust offset, check for the end of prices\n",
    "        and handle position change\n",
    "        :param action:\n",
    "        :return: reward, done\n",
    "        \"\"\"\n",
    "        assert isinstance(action, Actions)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        close = self._cur_close()\n",
    "        if action == Actions.Buy and not self.market_position:\n",
    "            self.market_position = True\n",
    "            self.open_price = close\n",
    "            reward -= self.open_price * self.commission_perc\n",
    "        elif action == Actions.Close and self.market_position:\n",
    "            self.market_position = False\n",
    "            reward -= close * self.commission_perc\n",
    "            if self.reward_on_close:\n",
    "                reward += close - self.open_price\n",
    "            self.open_price = 0.0\n",
    "            \n",
    "        self._offset += 1\n",
    "        prev_close = close\n",
    "        close = self._cur_close()\n",
    "        done |= self._offset >= self._states.shape[0]-1\n",
    "\n",
    "        if self.market_position and not self.reward_on_close:\n",
    "            reward += close - prev_close \n",
    "\n",
    "        return reward, done\n",
    "    def _cur_close(self):\n",
    "        \"\"\"\n",
    "        Calculate real close price for the current bar\n",
    "        \"\"\"\n",
    "        close = self._states.price[self._offset]\n",
    "        return close\n",
    "\n",
    "class StocksEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    spec = EnvSpec(\"StocksEnv-v0\")\n",
    "\n",
    "    def __init__(self, bars_count, commission, prices, states, random_ofs_on_reset, reward_on_close=True): \n",
    "        assert prices.shape[0] == states.shape[0]\n",
    "        self._prices = prices\n",
    "        self._states = states\n",
    "        self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "        self._state = State(states, bars_count, commission)\n",
    "        self.observation_space = self._state.space_type\n",
    "        self.n_states = self._state.n\n",
    "        self.random_ofs_on_reset = random_ofs_on_reset\n",
    "        self.seed()\n",
    "\n",
    "    def reset(self):\n",
    "        # make selection of the instrument and it's offset. Then reset the state\n",
    "        bars = self._state.bars_count\n",
    "        if self.random_ofs_on_reset:\n",
    "            offset = self.np_random.choice(\n",
    "                self._states.shape[0] - bars) + bars\n",
    "        else:\n",
    "            offset = bars\n",
    "        self._state.reset(self._states, offset)\n",
    "        return self._state.encode()\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = Actions(action_idx)\n",
    "        reward, done = self._state.step(action)\n",
    "        obs = self._state.encode()\n",
    "        info = {\n",
    "            \"offset\": self._state._offset\n",
    "        }\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2 ** 31\n",
    "        return [seed1, seed2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(\n",
    "            collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        states_list = [x[0] for x in self.transits.keys()]\n",
    "        for state in states_list :   \n",
    " #          print(state)\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "#            print(state_values)\n",
    "            self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_count = 1\n",
    "commission = 0.00075\n",
    "\n",
    "env = StocksEnv(bars_count, commission, prices, df_states, False)\n",
    "agent = Agent(env)\n",
    "test_env = StocksEnv(bars_count, commission, prices, df_states, False)\n",
    "\n",
    "writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "\n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "        print(reward)\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    if reward > best_reward:\n",
    "        print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "            best_reward, reward))\n",
    "        best_reward = reward\n",
    "    if reward > 800:\n",
    "        print(\"Solved in %d iterations!\" % iter_no)\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
